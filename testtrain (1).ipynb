{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import stft\n",
    "from scipy.fft import fft,fftfreq\n",
    "\n",
    "def get_fft_binned(input_waveform,min=0,max=5,sample_rate = 100,smoothing=1, bins=20):\n",
    "    # SUMMARY: Computes 1-D Fast Fourier Transform on a sequence of data, returns sum between min and max frequency in Hz normalized to waveform amplitude\n",
    "    # RETURNS: float (normalized total area under curve of fourier transform between min and max frequency specified)\n",
    "    # LIMITATIONS: Max frequency value is 1/2 of time interval. Utilize last 4 time intervals for more accurate fourier transform on higher frequencies.\n",
    "    flat = input_waveform.flatten()\n",
    "    yf = fft(flat)\n",
    "    xf = fftfreq(len(flat),100/len(flat))[:len(flat)//2]\n",
    "    yval = 2.0/len(flat)*np.abs(yf[:len(flat)//2])\n",
    "    s = pd.Series(yval)\n",
    "    y = s.rolling(smoothing)\n",
    "    avg = y.mean()\n",
    "\n",
    "    bin_values, bin_edges = np.histogram(avg[(xf>min) & (xf<max)], bins=bins)\n",
    "    return bin_values / bin_values.sum()\n",
    "    #sns.lineplot(x=xf,y=avg,linewidth=0.5) # plot waveform\n",
    "    #return avg[(xf>min) & (xf < max)].sum()/avg.sum()\n",
    "\n",
    "def linregress(data,time):\n",
    "    time_points = np.linspace(0, time, len(data))\n",
    "    slope, intercept = np.polyfit(time_points, data, 1)\n",
    "    return slope\n",
    "\n",
    "def get_input_vector(data,epoch,backward=5,bins_per=50):\n",
    "    #SUMMARY: Code generates an input vector given a dataset and epoch #\n",
    "    upperbounds = [30,30,20,1.0,1.0]\n",
    "    \n",
    "    output = np.zeros(int(5*bins_per+2))\n",
    "    if (epoch-5)<=0:\n",
    "        return output\n",
    "    for i in range(0,5):\n",
    "        # fetch data subset\n",
    "        subset = data[i,:,:]\n",
    "        # fetch epoch subset\n",
    "        subset = subset[(epoch-backward):epoch,:].flatten()\n",
    "        output[bins_per*i:bins_per*(i+1)] = get_fft_binned(subset,max=upperbounds[i],bins=bins_per)\n",
    "    output[5*bins_per] = np.mean(data[5,(epoch-backward):epoch,:].flatten())/40\n",
    "    output[5*bins_per+1] = linregress(data[5,(epoch-backward):epoch,:].flatten(),(backward + 1)/2)*50\n",
    "    return output\n",
    "\n",
    "def get_output_vector(labels,epoch):\n",
    "    # SUMMARY: Gets the output vector given an epoch #\n",
    "    output = np.zeros(6)\n",
    "    if (epoch-5)<=0:\n",
    "        return output\n",
    "    else:\n",
    "        #rint(labels.shape)\n",
    "        #print(epoch)\n",
    "        output[int(labels[epoch]-1)]=1\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_indices(input_data, input_labels, indices,num_per=40,bins_per=50):\n",
    "    in_vecs = np.zeros((len(indices),int(5*bins_per+2)))\n",
    "    out_vecs = np.zeros((len(indices),6))\n",
    "    \n",
    "    for i in range(0,len(indices)):\n",
    "        in_vecs[i,:] = get_input_vector(input_data,indices[i],bins_per=bins_per)\n",
    "        out_vecs[i,:] = get_output_vector(input_labels,indices[i])\n",
    "\n",
    "    return in_vecs, out_vecs\n",
    "\n",
    "\n",
    "def get_data_night(input_data, input_labels,num_per=40,bins_per=50):\n",
    "    #SUMMARY: Gets 40 input and output vectors for each sleep state given an input data and input labels\n",
    "    #RETURNS: input and ouput vectors \n",
    "    in_vectors = np.zeros((num_per*6,5*bins_per+2))\n",
    "    out_vectors = np.zeros((num_per*6,6))\n",
    "\n",
    "    for i in range(1,7):\n",
    "        if(len(np.where(input_labels == i)[0])<num_per):\n",
    "            indices = np.random.choice(np.where(input_labels == i)[0],size=len(np.where(input_labels == i)[0]))\n",
    "        else:\n",
    "            indices = np.random.choice(np.where(input_labels == i)[0],size=num_per,replace=False)\n",
    "        \n",
    "        while((indices<5).any()):\n",
    "            indices = np.random.choice(np.where(input_labels == i)[0],size=num_per,replace=False)\n",
    "        \n",
    "        in_vecs, out_vecs = get_data_indices(input_data,input_labels,indices,num_per,bins_per=bins_per)\n",
    "        \n",
    "        \n",
    "        if(len(np.where(input_labels == i)[0])<num_per):\n",
    "            count = len(np.where(input_labels == i)[0])\n",
    "            in_vectors[(i-1)*num_per:((i-1)*num_per+count),:] = in_vecs\n",
    "            out_vectors[(i-1)*num_per:((i-1)*num_per+count),:] = out_vecs\n",
    "        else:\n",
    "            in_vectors[(i-1)*num_per:(i)*num_per,:] = in_vecs\n",
    "            out_vectors[(i-1)*num_per:(i)*num_per,:] = out_vecs\n",
    "        \n",
    "    return in_vectors,out_vectors\n",
    "\n",
    "def get_data_night_alternate(input_data, input_labels,bins_per=50,padding=20):\n",
    "    # crop to data where we have > 0 for label (1 through 6 only)\n",
    "    input_data = input_data[:,input_labels>0,:]\n",
    "    input_labels = input_labels[input_labels>0]\n",
    "\n",
    "    # crop to first value where input is greater than 1 and last value of same positoin\n",
    "    total = len(input_labels)\n",
    "    min = np.arange(total)[input_labels>1].min()\n",
    "    max = np.arange(total)[input_labels>1].max()\n",
    "\n",
    "    input_data = input_data[:,(min-20):(max+20),:]\n",
    "    input_labels = input_labels[(min-20):(max+20)]\n",
    "\n",
    "    new_total = len(input_labels)\n",
    "\n",
    "    in_vecs, out_vecs = get_data_indices(input_data,input_labels,np.arange(new_total),bins_per=bins_per)\n",
    "        \n",
    "    return in_vecs,out_vecs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Testing and Training Data\n",
    "\n",
    "We generate training data using all data from the first 21 patients (excluding p23 and p22). \n",
    "\n",
    "We test our data on patients 22 and 23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "files = os.listdir(\"Data/\")\n",
    "# assume files are in order of (x,y)\n",
    "len(files) # all files are seen here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[311], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mreshape((data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]))\n\u001b[0;32m     11\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfiles[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28minput\u001b[39m, output \u001b[38;5;241m=\u001b[39m get_data_night_alternate(data,labels,bins_per\u001b[38;5;241m=\u001b[39mbins)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted Night \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(i\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m input_vectors[\u001b[38;5;28mint\u001b[39m(i\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n",
      "Cell \u001b[1;32mIn[309], line 55\u001b[0m, in \u001b[0;36mget_data_night_alternate\u001b[1;34m(input_data, input_labels, bins_per, padding)\u001b[0m\n\u001b[0;32m     51\u001b[0m input_labels \u001b[38;5;241m=\u001b[39m input_labels[(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m20\u001b[39m):(\u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m20\u001b[39m)]\n\u001b[0;32m     53\u001b[0m new_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_labels)\n\u001b[1;32m---> 55\u001b[0m in_vecs, out_vecs \u001b[38;5;241m=\u001b[39m get_data_indices(input_data,input_labels,np\u001b[38;5;241m.\u001b[39marange(new_total),bins_per\u001b[38;5;241m=\u001b[39mbins_per)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m in_vecs,out_vecs\n",
      "Cell \u001b[1;32mIn[309], line 6\u001b[0m, in \u001b[0;36mget_data_indices\u001b[1;34m(input_data, input_labels, indices, num_per, bins_per)\u001b[0m\n\u001b[0;32m      3\u001b[0m out_vecs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(indices),\u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(indices)):\n\u001b[1;32m----> 6\u001b[0m     in_vecs[i,:] \u001b[38;5;241m=\u001b[39m get_input_vector(input_data,indices[i],bins_per\u001b[38;5;241m=\u001b[39mbins_per)\n\u001b[0;32m      7\u001b[0m     out_vecs[i,:] \u001b[38;5;241m=\u001b[39m get_output_vector(input_labels,indices[i])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m in_vecs, out_vecs\n",
      "Cell \u001b[1;32mIn[308], line 42\u001b[0m, in \u001b[0;36mget_input_vector\u001b[1;34m(data, epoch, backward, bins_per)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# fetch epoch subset\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     subset \u001b[38;5;241m=\u001b[39m subset[(epoch\u001b[38;5;241m-\u001b[39mbackward):epoch,:]\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m---> 42\u001b[0m     output[bins_per\u001b[38;5;241m*\u001b[39mi:bins_per\u001b[38;5;241m*\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m=\u001b[39m get_fft_binned(subset,\u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mupperbounds[i],bins\u001b[38;5;241m=\u001b[39mbins_per)\n\u001b[0;32m     43\u001b[0m output[\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m*\u001b[39mbins_per] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(data[\u001b[38;5;241m5\u001b[39m,(epoch\u001b[38;5;241m-\u001b[39mbackward):epoch,:]\u001b[38;5;241m.\u001b[39mflatten())\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m40\u001b[39m\n\u001b[0;32m     44\u001b[0m output[\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m*\u001b[39mbins_per\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m linregress(data[\u001b[38;5;241m5\u001b[39m,(epoch\u001b[38;5;241m-\u001b[39mbackward):epoch,:]\u001b[38;5;241m.\u001b[39mflatten(),(backward \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m\n",
      "Cell \u001b[1;32mIn[308], line 20\u001b[0m, in \u001b[0;36mget_fft_binned\u001b[1;34m(input_waveform, min, max, sample_rate, smoothing, bins)\u001b[0m\n\u001b[0;32m     17\u001b[0m y \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mrolling(smoothing)\n\u001b[0;32m     18\u001b[0m avg \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m---> 20\u001b[0m bin_values, bin_edges \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhistogram(avg[(xf\u001b[38;5;241m>\u001b[39m\u001b[38;5;28mmin\u001b[39m) \u001b[38;5;241m&\u001b[39m (xf\u001b[38;5;241m<\u001b[39m\u001b[38;5;28mmax\u001b[39m)], bins\u001b[38;5;241m=\u001b[39mbins)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bin_values \u001b[38;5;241m/\u001b[39m bin_values\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32mc:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1005\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1003\u001b[0m     key \u001b[38;5;241m=\u001b[39m check_bool_indexer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, key)\n\u001b[0;32m   1004\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_values(key)\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_with(key)\n",
      "File \u001b[1;32mc:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_values\u001b[1;34m(self, indexer)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, indexer: \u001b[38;5;28mslice\u001b[39m \u001b[38;5;241m|\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mbool_]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m-> 1069\u001b[0m     new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mgetitem_mgr(indexer)\n\u001b[0;32m   1070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_mgr)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2034\u001b[0m, in \u001b[0;36mSingleBlockManager.getitem_mgr\u001b[1;34m(self, indexer)\u001b[0m\n\u001b[0;32m   2031\u001b[0m bp \u001b[38;5;241m=\u001b[39m BlockPlacement(\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(array)))\n\u001b[0;32m   2032\u001b[0m block \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(blk)(array, placement\u001b[38;5;241m=\u001b[39mbp, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 2034\u001b[0m new_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex[indexer]\n\u001b[0;32m   2035\u001b[0m \u001b[38;5;66;03m# TODO(CoW) in theory only need to track reference if new_array is a view\u001b[39;00m\n\u001b[0;32m   2036\u001b[0m ref \u001b[38;5;241m=\u001b[39m weakref\u001b[38;5;241m.\u001b[39mref(blk)\n",
      "File \u001b[1;32mc:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:979\u001b[0m, in \u001b[0;36mRangeIndex.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    972\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[0;32m    973\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly integers, slices (`:`), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    974\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mellipsis (`...`), numpy.newaxis (`None`) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    975\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand integer or boolean \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    976\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marrays are valid indices\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    977\u001b[0m     )\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# fall back to Int64Index\u001b[39;00m\n\u001b[1;32m--> 979\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(key)\n",
      "File \u001b[1;32mc:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5315\u001b[0m, in \u001b[0;36mIndex.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   5304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m   5305\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5306\u001b[0m \u001b[38;5;124;03m    Override numpy.ndarray's __getitem__ method to work as desired.\u001b[39;00m\n\u001b[0;32m   5307\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5313\u001b[0m \n\u001b[0;32m   5314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5315\u001b[0m     getitem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m\n\u001b[0;32m   5317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_integer(key) \u001b[38;5;129;01mor\u001b[39;00m is_float(key):\n\u001b[0;32m   5318\u001b[0m         \u001b[38;5;66;03m# GH#44051 exclude bool, which would return a 2d ndarray\u001b[39;00m\n\u001b[0;32m   5319\u001b[0m         key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mcast_scalar_indexer(key, warn_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\properties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:202\u001b[0m, in \u001b[0;36mRangeIndex._data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_data\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03m    An int array that for performance reasons is created only when needed.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m    The constructed array is saved in ``_cache``.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_per = 250\n",
    "bins = 20\n",
    "num_nights = 40\n",
    "\n",
    "input_vectors = np.empty(num_nights,dtype=object)\n",
    "output_vectors = np.empty(num_nights,dtype=object)\n",
    "\n",
    "for i in range(0,2*num_nights,2):\n",
    "    data = np.load(f\"Data/{files[i]}\")\n",
    "    data = data.reshape((data.shape[1],data.shape[0],data.shape[2]))\n",
    "    labels = np.load(f\"Data/{files[i+1]}\")\n",
    "    input, output = get_data_night_alternate(data,labels,bins_per=bins)\n",
    "    print(f\"Completed Night {(i/2)}\")\n",
    "    input_vectors[int(i/2)] = input\n",
    "    output_vectors[int(i/2)] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will acquire training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Night 0.0\n",
      "Completed Night 1.0\n",
      "Completed Night 2.0\n",
      "Completed Night 3.0\n",
      "Completed Night 4.0\n",
      "Completed Night 5.0\n",
      "Completed Night 6.0\n",
      "Completed Night 7.0\n",
      "Completed Night 8.0\n",
      "Completed Night 9.0\n",
      "Completed Night 10.0\n",
      "Completed Night 11.0\n",
      "Completed Night 12.0\n",
      "Completed Night 13.0\n",
      "Completed Night 14.0\n",
      "Completed Night 15.0\n",
      "Completed Night 16.0\n",
      "Completed Night 17.0\n",
      "Completed Night 18.0\n",
      "Completed Night 19.0\n",
      "Completed Night 20.0\n",
      "Completed Night 21.0\n",
      "Completed Night 22.0\n",
      "Completed Night 23.0\n",
      "Completed Night 24.0\n",
      "Completed Night 25.0\n",
      "Completed Night 26.0\n",
      "Completed Night 27.0\n",
      "Completed Night 28.0\n",
      "Completed Night 29.0\n",
      "Completed Night 30.0\n",
      "Completed Night 31.0\n",
      "Completed Night 32.0\n",
      "Completed Night 33.0\n",
      "Completed Night 34.0\n",
      "Completed Night 35.0\n",
      "Completed Night 36.0\n",
      "Completed Night 37.0\n",
      "Completed Night 38.0\n",
      "Completed Night 39.0\n"
     ]
    }
   ],
   "source": [
    "num_per = 250\n",
    "bins = 30\n",
    "num_nights = 40\n",
    "\n",
    "input_vectors = np.zeros((num_nights*6*num_per,5*bins+2))\n",
    "output_vectors = np.zeros((num_nights*6*num_per,6))\n",
    "\n",
    "for i in range(0,2*num_nights,2):\n",
    "    data = np.load(f\"Data/{files[i]}\")\n",
    "    data = data.reshape((data.shape[1],data.shape[0],data.shape[2]))\n",
    "    labels = np.load(f\"Data/{files[i+1]}\")\n",
    "\n",
    "    input, output = get_data_night(data,labels,num_per=num_per,bins_per=bins)\n",
    "    input_vectors[int(6*num_per*(i/2)):int((6*num_per*((i/2)+1))),:] = input\n",
    "    output_vectors[int(6*num_per*(i/2)):int((6*num_per*((i/2)+1))),:] = output\n",
    "    print(f\"Completed Night {(i/2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = input_vectors.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(input_vectors)):\n",
    "    input_vectors[i] = input_vectors[i][np.mean(input_vectors[i],axis=1)!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup2 = output_vectors.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(output_vectors)):\n",
    "    output_vectors[i] = output_vectors[i][np.mean(output_vectors[i],axis=1)!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"sequential_data_input.npy\",input_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"sequential_data_output.npy\",output_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "754"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vectors = input_vectors[np.mean(input_vectors,axis=1)!=0]\n",
    "output_vectors = output_vectors[np.mean(output_vectors,axis=1)!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"train_X.npy\",input_vectors)\n",
    "np.save(\"train_Y.npy\",output_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.mean(input_vectors,axis=1)==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35838, 152)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(input_vecs,output_vecs,split=0.9):\n",
    "    total = len(output_vecs)\n",
    "    train_total = int(np.round(total*split))\n",
    "    test_total = total-train_total\n",
    "\n",
    "    indices_train = np.random.choice(int(total),size=train_total,replace=False)\n",
    "    # for getting test indices\n",
    "    all = pd.Series(np.arange(total))\n",
    "    indices_test = all[all.isin(indices_train)==False].values\n",
    "\n",
    "    train_X = input_vecs[indices_train,:]\n",
    "    test_X = input_vecs[indices_test,:]\n",
    "    \n",
    "    train_Y = output_vecs[indices_train,:]\n",
    "    test_Y = output_vecs[indices_test,:]\n",
    "\n",
    "    return train_X, train_Y, test_X, test_Y\n",
    "    \n",
    "train_X, train_Y, test_X, test_Y = train_test_split(input_vectors,output_vectors,split=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.load(\"train_input.npy\")\n",
    "train_Y = np.load(\"train_output.npy\")\n",
    "\n",
    "test_X = np.load(\"test_input.npy\")\n",
    "test_Y = np.load(\"test_output.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4347, 252)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.54405496\n",
      "Iteration 2, loss = 1.67558140\n",
      "Iteration 3, loss = 1.43565531\n",
      "Iteration 4, loss = 1.33542519\n",
      "Iteration 5, loss = 1.27124212\n",
      "Iteration 6, loss = 1.20863482\n",
      "Iteration 7, loss = 1.16823833\n",
      "Iteration 8, loss = 1.13113055\n",
      "Iteration 9, loss = 1.10952048\n",
      "Iteration 10, loss = 1.08833113\n",
      "Iteration 11, loss = 1.06656977\n",
      "Iteration 12, loss = 1.05097545\n",
      "Iteration 13, loss = 1.04887798\n",
      "Iteration 14, loss = 1.03769697\n",
      "Iteration 15, loss = 1.01993697\n",
      "Iteration 16, loss = 1.01306597\n",
      "Iteration 17, loss = 1.00287208\n",
      "Iteration 18, loss = 0.98709053\n",
      "Iteration 19, loss = 0.98681248\n",
      "Iteration 20, loss = 0.97769920\n",
      "Iteration 21, loss = 0.96969929\n",
      "Iteration 22, loss = 0.96296226\n",
      "Iteration 23, loss = 0.95662177\n",
      "Iteration 24, loss = 0.94741654\n",
      "Iteration 25, loss = 0.94095047\n",
      "Iteration 26, loss = 0.93611155\n",
      "Iteration 27, loss = 0.92826728\n",
      "Iteration 28, loss = 0.92538701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=0.001, hidden_layer_sizes=(250, 100, 50, 30, 15),\n",
       "              max_iter=1000, n_iter_no_change=30, random_state=1, tol=1e-05,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.001, hidden_layer_sizes=(250, 100, 50, 30, 15),\n",
       "              max_iter=1000, n_iter_no_change=30, random_state=1, tol=1e-05,\n",
       "              verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(alpha=0.001, hidden_layer_sizes=(250, 100, 50, 30, 15),\n",
       "              max_iter=1000, n_iter_no_change=30, random_state=1, tol=1e-05,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(train_X)\n",
    "#train_X_scaled = scaler.transform(train_X)\n",
    "train_X_scaled=train_X\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(250,100,50,30,15),\n",
    "                    activation='relu',\n",
    "                    solver='adam',\n",
    "                    alpha=0.001,\n",
    "                    learning_rate='constant',\n",
    "                    learning_rate_init=0.001,\n",
    "                    max_iter=1000,\n",
    "                    random_state=1,\n",
    "                    verbose=True,tol=0.00001,\n",
    "                    n_iter_no_change=30)\n",
    "\n",
    "mlp.fit(train_X_scaled,train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will acquire testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p22_n1_X.npy', 'p22_n1_y.npy', 'p22_n2_X.npy', 'p22_n2_y.npy', 'p23_n1_X.npy', 'p23_n1_y.npy']\n",
      "Completed Night 0.0\n",
      "Completed Night 1.0\n",
      "Completed Night 2.0\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(\"Data/\")\n",
    "files = files[80:86] # last 3 nights only\n",
    "print(files)\n",
    "\n",
    "num_nights = int(len(files)/2)\n",
    "\n",
    "num_per = 250\n",
    "bins = 20\n",
    "\n",
    "input_vectors = np.empty(num_nights,dtype=object)\n",
    "output_vectors = np.empty(num_nights,dtype=object)\n",
    "\n",
    "for i in range(0,2*num_nights,2):\n",
    "    data = np.load(f\"Data/{files[i]}\")\n",
    "    data = data.reshape((data.shape[1],data.shape[0],data.shape[2]))\n",
    "    labels = np.load(f\"Data/{files[i+1]}\")\n",
    "    input, output = get_data_night_alternate(data,labels,bins_per=bins)\n",
    "    print(f\"Completed Night {(i/2)}\")\n",
    "    input_vectors[int(i/2)] = input\n",
    "    output_vectors[int(i/2)] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1041, 102)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(\"sequential_data_input.npy\",allow_pickle=True)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(input_vectors)):\n",
    "    input_vectors[i] = input_vectors[i][np.mean(input_vectors[i],axis=1)!=0]\n",
    "\n",
    "for i in range(0,len(output_vectors)):\n",
    "    output_vectors[i] = output_vectors[i][np.mean(output_vectors[i],axis=1)!=0]\n",
    "\n",
    "np.save(\"TEST_sequential_data_output.npy\",output_vectors)\n",
    "np.save(\"TEST_sequential_data_input.npy\",input_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p22_n1_X.npy', 'p22_n1_y.npy', 'p22_n2_X.npy', 'p22_n2_y.npy', 'p23_n1_X.npy', 'p23_n1_y.npy']\n",
      "Completed Night 0.0\n",
      "Completed Night 1.0\n",
      "Completed Night 2.0\n"
     ]
    }
   ],
   "source": [
    "num_per=250\n",
    "\n",
    "files = os.listdir(\"Data/\")\n",
    "files = files[80:86] # last 3 nights only\n",
    "print(files)\n",
    "\n",
    "num_nights = int(len(files)/2)\n",
    "\n",
    "num_per = 20\n",
    "bins_per = 30\n",
    "num_nights = 40\n",
    "\n",
    "input_vectors = np.zeros((num_nights*6*num_per,5*bins_per+2))\n",
    "output_vectors = np.zeros((num_nights*6*num_per,6))\n",
    "\n",
    "for i in range(0,len(files),2):\n",
    "    data = np.load(f\"Data/{files[i]}\")\n",
    "    data = data.reshape((data.shape[1],data.shape[0],data.shape[2]))\n",
    "    labels = np.load(f\"Data/{files[i+1]}\")\n",
    "\n",
    "    input, output = get_data_night(data,labels,num_per=num_per,bins_per=bins)\n",
    "    input_vectors[int(6*num_per*(i/2)):int((6*num_per*((i/2)+1))),:] = input\n",
    "    output_vectors[int(6*num_per*(i/2)):int((6*num_per*((i/2)+1))),:] = output\n",
    "    print(f\"Completed Night {(i/2)}\")\n",
    "\n",
    "input_vectors = input_vectors[np.mean(input_vectors,axis=1)!=0]\n",
    "output_vectors = output_vectors[np.mean(output_vectors,axis=1)!=0]\n",
    "\n",
    "test_X = input_vectors\n",
    "test_Y = output_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4171974522292994"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_X_scaled = scaler.transform(test_X)\n",
    "test_X_scaled = test_X\n",
    "mlp.score(test_X_scaled,test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"test_input.npy\",test_X)\n",
    "np.save(\"train_input.npy\",train_X)\n",
    "np.save(\"test_output.npy\",test_Y)\n",
    "np.save(\"train_output.npy\",train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86666667 0.4        0.76666667 0.17307692 0.         0.35      ]\n"
     ]
    }
   ],
   "source": [
    "correct = np.zeros(6)\n",
    "incorrect = np.zeros(6)\n",
    "\n",
    "for i in range(0,len(test_X_scaled)):\n",
    "    predictedprob = mlp.predict_proba(test_X_scaled[i,:].reshape(1,-1))\n",
    "    predicted = (predictedprob==np.max(predictedprob)).astype(np.uint8)\n",
    "    #print(predicted)\n",
    "    if((test_Y[i].astype(np.int32)==predicted[0]).all()):\n",
    "        correct = correct + test_Y[i]\n",
    "    else:\n",
    "        incorrect = incorrect + test_Y[i]\n",
    "\n",
    "print(correct/(correct+incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10000.,  2935.,  9967.,  3244.,  2256.,  7436.])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(train_Y,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4840764331210191"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(correct)/(np.sum(correct+incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input_vectors_noncorrupted_even_more.npy',input_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('output_vectors_noncorrupted_even_more.npy',output_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N x 721 x 102\n",
    "output = np.zeros((3,721,102))\n",
    "for i in range(0,len(input_vectors)):\n",
    "    output[i,:,:] = input_vectors[i][0:721,:]\n",
    "\n",
    "np.save(\"test_input_3d.npy\",output)\n",
    "\n",
    "# N x 721 x 102\n",
    "out2 = np.zeros((3,721,6))\n",
    "for i in range(0,len(output_vectors)):\n",
    "    out2[i,:,:] = output_vectors[i][0:721,:]\n",
    "\n",
    "np.save(\"test_output_3d.npy\",out2)\n",
    "\n",
    "out2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vectors = np.load(\"sequential_data_input.npy\",allow_pickle=True)\n",
    "output_vectors = np.load(\"sequential_data_output.npy\",allow_pickle=True)\n",
    "\n",
    "sizes = np.zeros(40)\n",
    "for i in range(0,40):\n",
    "    sizes[i] = input_vectors[i].shape[0]\n",
    "\n",
    "input_vectors = input_vectors[sizes>720]\n",
    "output_vectors = output_vectors[sizes>720]\n",
    "\n",
    "\n",
    "# N x 721 x 102\n",
    "output = np.zeros((38,721,102))\n",
    "for i in range(0,len(input_vectors)):\n",
    "    output[i,:,:] = input_vectors[i][0:721,:]\n",
    "\n",
    "np.save(\"train_input_3d.npy\",output)\n",
    "\n",
    "# N x 721 x 102\n",
    "out2 = np.zeros((38,721,6))\n",
    "for i in range(0,len(output_vectors)):\n",
    "    out2[i,:,:] = output_vectors[i][0:721,:]\n",
    "\n",
    "np.save(\"train_output_3d.npy\",out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3005.32435585\n",
      "Iteration 2, loss = 2713.37875579\n",
      "Iteration 3, loss = 2242.78840589\n",
      "Iteration 4, loss = 1909.87977123\n",
      "Iteration 5, loss = 1728.79735169\n",
      "Iteration 6, loss = 1633.02751246\n",
      "Iteration 7, loss = 1601.46453560\n",
      "Iteration 8, loss = 1587.81876251\n",
      "Iteration 9, loss = 1572.97297591\n",
      "Iteration 10, loss = 1554.08844651\n",
      "Iteration 11, loss = 1530.23430250\n",
      "Iteration 12, loss = 1505.89927193\n",
      "Iteration 13, loss = 1479.59183885\n",
      "Iteration 14, loss = 1453.90687737\n",
      "Iteration 15, loss = 1425.74610036\n",
      "Iteration 16, loss = 1397.39497195\n",
      "Iteration 17, loss = 1366.75834675\n",
      "Iteration 18, loss = 1333.41497317\n",
      "Iteration 19, loss = 1298.16961201\n",
      "Iteration 20, loss = 1261.97382098\n",
      "Iteration 21, loss = 1226.73969513\n",
      "Iteration 22, loss = 1192.51166413\n",
      "Iteration 23, loss = 1158.64710132\n",
      "Iteration 24, loss = 1125.91075166\n",
      "Iteration 25, loss = 1093.03021467\n",
      "Iteration 26, loss = 1058.83954738\n",
      "Iteration 27, loss = 1023.78902573\n",
      "Iteration 28, loss = 988.08951630\n",
      "Iteration 29, loss = 950.99820452\n",
      "Iteration 30, loss = 913.70901637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-18 {color: black;background-color: white;}#sk-container-id-18 pre{padding: 0;}#sk-container-id-18 div.sk-toggleable {background-color: white;}#sk-container-id-18 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-18 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-18 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-18 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-18 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-18 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-18 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-18 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-18 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-18 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-18 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-18 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-18 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-18 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-18 div.sk-item {position: relative;z-index: 1;}#sk-container-id-18 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-18 div.sk-item::before, #sk-container-id-18 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-18 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-18 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-18 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-18 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-18 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-18 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-18 div.sk-label-container {text-align: center;}#sk-container-id-18 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-18 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-18\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=0.001, hidden_layer_sizes=(1024, 512),\n",
       "              learning_rate=&#x27;adaptive&#x27;, max_iter=30, n_iter_no_change=30,\n",
       "              random_state=1, tol=1e-05, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" checked><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.001, hidden_layer_sizes=(1024, 512),\n",
       "              learning_rate=&#x27;adaptive&#x27;, max_iter=30, n_iter_no_change=30,\n",
       "              random_state=1, tol=1e-05, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(alpha=0.001, hidden_layer_sizes=(1024, 512),\n",
       "              learning_rate='adaptive', max_iter=30, n_iter_no_change=30,\n",
       "              random_state=1, tol=1e-05, verbose=True)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(train_X)\n",
    "#train_X_scaled = scaler.transform(train_X)\n",
    "train_X = output.reshape((38,721*102))\n",
    "train_Y = out2.reshape((38,721*6))\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1024,512),\n",
    "                    activation='relu',\n",
    "                    solver='adam',\n",
    "                    alpha=0.001,\n",
    "                    learning_rate='adaptive',\n",
    "                    learning_rate_init=0.001,\n",
    "                    max_iter=30,\n",
    "                    random_state=1,\n",
    "                    verbose=True,tol=0.00001,\n",
    "                    n_iter_no_change=30)\n",
    "\n",
    "mlp.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.load(\"test_input_3d.npy\").reshape(3,721*102)\n",
    "test_Y = np.load(\"test_output_3d.npy\").reshape(3,721*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3079056865464632\n",
      "0.46601941747572817\n",
      "0.5339805825242718\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(test_X)):\n",
    "    predicted = mlp.predict_proba(test_X[i].reshape(1, -1))\n",
    "    \n",
    "    reshaped = predicted.reshape((721,6))\n",
    "    # take max\n",
    "    output = np.zeros((721,6))\n",
    "    for j in range(0,721):\n",
    "        output[j] = reshaped[j]==np.max(reshaped[j])\n",
    "    success = 0\n",
    "    fail = 0\n",
    "    truth = test_Y[i].reshape((721,6))\n",
    "\n",
    "    for j in range(0,721):\n",
    "        if((output[j,:]==truth[j,:]).all()):\n",
    "            success = success + 1\n",
    "        else:\n",
    "            fail = fail + 1\n",
    "    print(success/(success+fail))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
